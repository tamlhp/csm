{\rtf1\ansi\uc0\deff0{\fonttbl{\f0\fmodern\fprq1\fcharset0;}}{\colortbl;\red136\green136\blue136;\red186\green33\blue33;\red0\green68\blue221;\red102\green102\blue102;\red64\green128\blue128;\red160\green160\blue0;\red25\green23\blue124;\red0\green128\blue0;\red187\green102\blue136;\red187\green102\blue34;\red136\green0\blue0;\red170\green34\blue255;\red153\green153\blue153;\red0\green160\blue0;\red160\green0\blue0;\red255\green0\blue0;\red128\green0\blue128;\red176\green0\blue64;\red0\green0\blue255;\red187\green187\blue187;\red188\green122\blue0;\red0\green0\blue128;\red125\green144\blue41;\red210\green65\blue58;}\f0 {\cf8\b from} {\cf19\b __future__} {\cf8\b import} division\par
{\cf8\b import} {\cf19\b re}\par
{\cf8\b import} {\cf19\b json}\par
{\cf8\b import} {\cf19\b operator}\par
{\cf8\b from} {\cf19\b collections} {\cf8\b import} Counter\par
{\cf8\b from} {\cf19\b nltk.corpus} {\cf8\b import} stopwords\par
{\cf8\b from} {\cf19\b nltk.tokenize} {\cf8\b import} word_tokenize\par
{\cf8\b from} {\cf19\b nltk} {\cf8\b import} bigrams\par
{\cf8\b from} {\cf19\b collections} {\cf8\b import} defaultdict\par
{\cf8\b import} {\cf19\b string}\par
{\cf8\b import} {\cf19\b vincent}\par
{\cf8\b import} {\cf19\b pandas}\par
{\cf8\b from} {\cf19\b datetime} {\cf8\b import} datetime, timedelta\par
{\cf8\b import} {\cf19\b math}\par
 \par
emoticons_str {\cf4 =} {\cf2 r"""}{\cf2 \par
}{\cf2     (?:}{\cf2 \par
}{\cf2         [:=;] # Eyes}{\cf2 \par
}{\cf2         [oO}{\cf2 \\}{\cf2 -]? # Nose (optional)}{\cf2 \par
}{\cf2         [D}{\cf2 \\}{\cf2 )}{\cf2 \\}{\cf2 ]}{\cf2 \\}{\cf2 (}{\cf2 \\}{\cf2 ]/}{\cf2 \\}{\cf2 \\}{\cf2 OpP] # Mouth}{\cf2 \par
}{\cf2     )}{\cf2 """}\par
 \par
regex_str {\cf4 =} [\par
    emoticons_str,\par
    {\cf2 r'}{\cf2 <[^>]+>}{\cf2 '}, {\cf5\i # HTML tags}\par
    {\cf2 r'}{\cf2 (?:@[}{\cf2 \\}{\cf2 w_]+)}{\cf2 '}, {\cf5\i # @-mentions}\par
    {\cf2 r"}{\cf2 (?:}{\cf2 \\}{\cf2 #+[}{\cf2 \\}{\cf2 w_]+[}{\cf2 \\}{\cf2 w}{\cf2 \\}{\cf2 '}{\cf2 _}{\cf2 \\}{\cf2 -]*[}{\cf2 \\}{\cf2 w_]+)}{\cf2 "}, {\cf5\i # hash-tags}\par
    {\cf2 r'}{\cf2 http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*}{\cf2 \\}{\cf2 (}{\cf2 \\}{\cf2 ),]|(?:}{\cf2 %}{\cf2 [0-9a-f][0-9a-f]))+}{\cf2 '}, {\cf5\i # URLs}\par
 \par
    {\cf2 r'}{\cf2 (?:(?:}{\cf2 \\}{\cf2 d+,?)+(?:}{\cf2 \\}{\cf2 .?}{\cf2 \\}{\cf2 d+)?)}{\cf2 '}, {\cf5\i # numbers}\par
    {\cf2 r"}{\cf2 (?:[a-z][a-z}{\cf2 '}{\cf2 \\}{\cf2 -_]+[a-z])}{\cf2 "}, {\cf5\i # words with - and '}\par
    {\cf2 r'}{\cf2 (?:[}{\cf2 \\}{\cf2 w_]+)}{\cf2 '}, {\cf5\i # other words}\par
    {\cf2 r'}{\cf2 (?:}{\cf2 \\}{\cf2 S)}{\cf2 '} {\cf5\i # anything else}\par
]\par
    \par
tokens_re {\cf4 =} re{\cf4 .}compile({\cf2 r'}{\cf2 (}{\cf2 '}{\cf4 +}{\cf2 '}{\cf2 |}{\cf2 '}{\cf4 .}join(regex_str){\cf4 +}{\cf2 '}{\cf2 )}{\cf2 '}, re{\cf4 .}VERBOSE {\cf4 |} re{\cf4 .}IGNORECASE)\par
emoticon_re {\cf4 =} re{\cf4 .}compile({\cf2 r'}{\cf2 ^}{\cf2 '}{\cf4 +}emoticons_str{\cf4 +}{\cf2 '}{\cf2 $}{\cf2 '}, re{\cf4 .}VERBOSE {\cf4 |} re{\cf4 .}IGNORECASE)\par
\par
punctuation {\cf4 =} {\cf8 list}(string{\cf4 .}punctuation)\par
stop {\cf4 =} stopwords{\cf4 .}words({\cf2 '}{\cf2 english}{\cf2 '}) {\cf4 +} punctuation {\cf4 +} [{\cf2 '}{\cf2 rt}{\cf2 '}, {\cf2 '}{\cf2 via}{\cf2 '}]\par
 \par
{\cf8\b def} {\cf19 tokenize}(s):\par
    {\cf5\i #return word_tokenize(tweet)}\par
    {\cf8\b return} tokens_re{\cf4 .}findall(s)\par
 \par
{\cf8\b def} {\cf19 preprocess}(s, lowercase{\cf4 =}{\cf8 False}):\par
    tokens {\cf4 =} tokenize(s)\par
    {\cf8\b if} lowercase:\par
        tokens {\cf4 =} [token {\cf8\b if} emoticon_re{\cf4 .}search(token) {\cf8\b else} token{\cf4 .}lower() {\cf8\b for} token {\cf12\b in} tokens]\par
    {\cf8\b return} tokens\par
\par
{\cf8\b def} {\cf19 terms_all}(terms): {\cf8\b return} [term {\cf8\b for} term {\cf12\b in} terms]\par
{\cf8\b def} {\cf19 terms_stop}(terms): {\cf8\b return} [term {\cf8\b for} term {\cf12\b in} terms {\cf8\b if} term {\cf12\b not} {\cf12\b in} stop]\par
{\cf8\b def} {\cf19 terms_bigram}(terms): {\cf8\b return} bigrams(terms)\par
\par
{\cf5\i # Count terms only once, equivalent to Document Frequency}\par
{\cf8\b def} {\cf19 terms_single}(terms): {\cf8\b return} {\cf8 set}(terms)\par
\par
{\cf5\i # Count hashtags only}\par
{\cf8\b def} {\cf19 terms_hash}(terms): {\cf8\b return} [term {\cf8\b for} term {\cf12\b in} terms {\cf8\b if} term{\cf4 .}startswith({\cf2 '}{\cf2 #}{\cf2 '})]\par
\par
{\cf5\i # Count terms only (no hashtags, no mentions)}\par
{\cf8\b def} {\cf19 terms_only}(terms):\par
    {\cf5\i # mind the ((double brackets))}\par
    {\cf5\i # startswith() takes a tuple (not a list) if }\par
    {\cf5\i # we pass a list of inputs}\par
    {\cf8\b return} [term {\cf8\b for} term {\cf12\b in} terms {\cf8\b if} term {\cf12\b not} {\cf12\b in} stop {\cf12\b and} {\cf12\b not} term{\cf4 .}startswith(({\cf2 '}{\cf2 #}{\cf2 '}, {\cf2 '}{\cf2 @}{\cf2 '}))] \par
\par
{\cf8\b def} {\cf19 terms_nolink}(terms):\par
    {\cf8\b return} [term {\cf8\b for} term {\cf12\b in} terms {\cf8\b if} {\cf2 '}{\cf2 http}{\cf2 '} {\cf12\b not} {\cf12\b in} term] \par
\par
{\cf8\b def} {\cf19 terms_filter}(tokens):\par
    {\cf5\i #terms = terms_hash(tokens)}\par
    terms {\cf4 =} terms_nolink(terms_stop(terms_only(tokens)))\par
    {\cf5\i #terms = terms_bigram(terms)}\par
    {\cf8\b return} terms\par
\par
{\cf8\b def} {\cf19 co_occurrences}(terms, com):\par
    {\cf8\b if} com {\cf12\b is} {\cf8 None}: com {\cf4 =} defaultdict({\cf8\b lambda} : defaultdict({\cf8 int}))\par
\par
    {\cf8\b for} i {\cf12\b in} {\cf8 range}({\cf8 len}(terms){\cf4 -}{\cf4 1}):            \par
        {\cf8\b for} j {\cf12\b in} {\cf8 range}(i{\cf4 +}{\cf4 1}, {\cf8 len}(terms)):\par
            w1, w2 {\cf4 =} {\cf8 sorted}([terms[i], terms[j]])                \par
            {\cf8\b if} w1 {\cf4 !=} w2:\par
                com[w1][w2] {\cf4 +}{\cf4 =} {\cf4 1}\par
\par
{\cf8\b def} {\cf19 contains_search_word}(terms, search_word):\par
    {\cf8\b if} search_word {\cf4 ==} {\cf2 '}{\cf2 '} {\cf12\b or} search_word {\cf12\b in} terms:\par
        {\cf8\b return} terms\par
    {\cf8\b else}:\par
        {\cf8\b return} []\par
\par
{\cf8\b def} {\cf19 getTime}(tweet):\par
    tweetDate {\cf4 =} tweet{\cf4 .}get({\cf2 '}{\cf2 created_at}{\cf2 '}, {\cf8 None})\par
    utcOffset {\cf4 =} tweet{\cf4 .}get({\cf2 '}{\cf2 user}{\cf2 '}){\cf4 .}get({\cf2 '}{\cf2 utc_offset}{\cf2 '})\par
\par
    {\cf8\b if} utcOffset {\cf12\b is} {\cf12\b not} {\cf8 None}:\par
        tweetCorrectedDate {\cf4 =} datetime{\cf4 .}strptime(tweetDate, {\cf2 '}{\cf2 %}{\cf2 a }{\cf2 %}{\cf2 b }{\cf9\b %d}{\cf2  }{\cf2 %}{\cf2 H:}{\cf2 %}{\cf2 M:}{\cf2 %}{\cf2 S +0000 }{\cf2 %}{\cf2 Y}{\cf2 '}) {\cf4 +} timedelta(seconds{\cf4 =}{\cf8 int}(utcOffset))\par
    {\cf8\b else}:\par
        tweetCorrectedDate {\cf4 =} datetime{\cf4 .}strptime(tweetDate, {\cf2 '}{\cf2 %}{\cf2 a }{\cf2 %}{\cf2 b }{\cf9\b %d}{\cf2  }{\cf2 %}{\cf2 H:}{\cf2 %}{\cf2 M:}{\cf2 %}{\cf2 S +0000 }{\cf2 %}{\cf2 Y}{\cf2 '})\par
\par
    {\cf8\b return} tweetCorrectedDate\par
\par
{\cf5\i # For each term, look for the most common co-occurrent terms}\par
{\cf8\b def} {\cf19 terms_max}(com):\par
    com_max {\cf4 =} []\par
    {\cf8\b for} t1 {\cf12\b in} com:\par
        t1_max_terms {\cf4 =} {\cf8 sorted}(com[t1]{\cf4 .}items(), key{\cf4 =}operator{\cf4 .}itemgetter({\cf4 1}), reverse{\cf4 =}{\cf8 True})[:{\cf4 5}]\par
        {\cf8\b for} t2, t2_count {\cf12\b in} t1_max_terms:\par
            com_max{\cf4 .}append(((t1, t2), t2_count))\par
    {\cf5\i # Get the most frequent co-occurrences}\par
    terms_max {\cf4 =} {\cf8 sorted}(com_max, key{\cf4 =}operator{\cf4 .}itemgetter({\cf4 1}), reverse{\cf4 =}{\cf8 True})\par
    {\cf8\b return} terms_max\par
\par
{\cf8\b def} {\cf19 time_series}(dates, period{\cf4 =}{\cf2 '}{\cf2 1Min}{\cf2 '}, counts{\cf4 =}{\cf8 None}):\par
    {\cf5\i # a list of "1" to count the hashtags}\par
    {\cf8\b if} count {\cf12\b is} {\cf8 None}: count {\cf4 =} [{\cf4 1}]{\cf4 *}{\cf8 len}(dates)\par
    {\cf5\i # the index of the series}\par
    idx {\cf4 =} pandas{\cf4 .}DatetimeIndex(dates)\par
    {\cf5\i # the actual series (at series of 1s for the moment)}\par
    series {\cf4 =} pandas{\cf4 .}Series(counts, index{\cf4 =}idx)\par
    per_minute {\cf4 =} series{\cf4 .}resample(period, how{\cf4 =}{\cf2 '}{\cf2 sum}{\cf2 '}){\cf4 .}fillna({\cf4 0})\par
    {\cf8\b return} per_minute\par
\par
{\cf8\b def} {\cf19 sentiment}(count, com, n_docs, pos_file {\cf4 =} {\cf8 None}, neg_file {\cf4 =} {\cf8 None}):\par
    {\cf8\b if} pos_file {\cf12\b is} {\cf8 None}:\par
        positive_vocab {\cf4 =} [\par
            {\cf2 '}{\cf2 good}{\cf2 '}, {\cf2 '}{\cf2 nice}{\cf2 '}, {\cf2 '}{\cf2 great}{\cf2 '}, {\cf2 '}{\cf2 awesome}{\cf2 '}, {\cf2 '}{\cf2 outstanding}{\cf2 '},\par
            {\cf2 '}{\cf2 fantastic}{\cf2 '}, {\cf2 '}{\cf2 terrific}{\cf2 '}, {\cf2 '}{\cf2 :)}{\cf2 '}, {\cf2 '}{\cf2 :-)}{\cf2 '}, {\cf2 '}{\cf2 like}{\cf2 '}, {\cf2 '}{\cf2 love}{\cf2 '},\par
            {\cf5\i # shall we also include game-specific terms?}\par
            {\cf5\i # 'triumph', 'triumphal', 'triumphant', 'victory', etc.}\par
        ]\par
    {\cf8\b else}:\par
        positive_vocab {\cf4 =} [line{\cf4 .}strip() {\cf8\b for} line {\cf12\b in} {\cf8 open}(pos_file)]\par
    \par
    {\cf8\b if} neg_file {\cf12\b is} {\cf8 None}:\par
        negative_vocab {\cf4 =} [\par
            {\cf2 '}{\cf2 bad}{\cf2 '}, {\cf2 '}{\cf2 terrible}{\cf2 '}, {\cf2 '}{\cf2 crap}{\cf2 '}, {\cf2 '}{\cf2 useless}{\cf2 '}, {\cf2 '}{\cf2 hate}{\cf2 '}, {\cf2 '}{\cf2 :(}{\cf2 '}, {\cf2 '}{\cf2 :-(}{\cf2 '},\par
            {\cf5\i # 'defeat', etc.}\par
        ]\par
    {\cf8\b else}:\par
        negative_vocab {\cf4 =} [line{\cf4 .}strip() {\cf8\b for} line {\cf12\b in} {\cf8 open}(neg_file)]\par
\par
    {\cf5\i # n_docs is the total n. of tweets}\par
    p_t {\cf4 =} \{\}\par
    p_t_com {\cf4 =} defaultdict({\cf8\b lambda} : defaultdict({\cf8 int}))\par
     \par
    {\cf8\b for} term, n {\cf12\b in} count{\cf4 .}items():\par
        p_t[term] {\cf4 =} n {\cf4 /} n_docs\par
        {\cf8\b for} t2 {\cf12\b in} com[term]:\par
            p_t_com[term][t2] {\cf4 =} com[term][t2] {\cf4 /} n_docs\par
\par
    pmi {\cf4 =} defaultdict({\cf8\b lambda} : defaultdict({\cf8 int}))\par
    {\cf8\b for} t1 {\cf12\b in} p_t:\par
        {\cf8\b for} t2 {\cf12\b in} com[t1]:\par
            denom {\cf4 =} p_t[t1] {\cf4 *} p_t[t2]\par
            pmi[t1][t2] {\cf4 =} math{\cf4 .}log(p_t_com[t1][t2] {\cf4 /} denom, {\cf4 2})\par
     \par
    semantic_orientation {\cf4 =} \{\}\par
    {\cf8\b for} term, n {\cf12\b in} p_t{\cf4 .}items():\par
        positive_assoc {\cf4 =} {\cf8 sum}(pmi[term][tx] {\cf8\b for} tx {\cf12\b in} positive_vocab)\par
        negative_assoc {\cf4 =} {\cf8 sum}(pmi[term][tx] {\cf8\b for} tx {\cf12\b in} negative_vocab)\par
        semantic_orientation[term] {\cf4 =} positive_assoc {\cf4 -} negative_assoc\par
\par
    semantic_sorted {\cf4 =} {\cf8 sorted}(semantic_orientation{\cf4 .}items(), \par
                         key{\cf4 =}operator{\cf4 .}itemgetter({\cf4 1}), \par
                         reverse{\cf4 =}{\cf8 True})\par
    {\cf8\b return} semantic_sorted\par
\par
{\cf8\b def} {\cf19 main}(fn):\par
    {\cf8\b with} {\cf8 open}(fn {\cf4 +} {\cf2 '}{\cf2 .json}{\cf2 '}, {\cf2 '}{\cf2 r}{\cf2 '}) {\cf8\b as} f:\par
        count_all {\cf4 =} Counter()\par
        com {\cf4 =} defaultdict({\cf8\b lambda} : defaultdict({\cf8 int}))\par
        search_word {\cf4 =} {\cf2 '}{\cf2 '}\par
\par
        {\cf8\b for} line {\cf12\b in} f:\par
            tweet {\cf4 =} json{\cf4 .}loads(line)\par
            tokens {\cf4 =} preprocess(tweet[{\cf2 '}{\cf2 text}{\cf2 '}])\par
            \par
            terms {\cf4 =} terms_filter(tokens)\par
\par
            co_occurrences(terms, com)\par
\par
            terms {\cf4 =} contains_search_word(terms, search_word)\par
            count_all{\cf4 .}update(terms)\par
            {\cf5\i #print tokens}\par
    {\cf8\b print}(count_all{\cf4 .}most_common({\cf4 10}))        \par
    {\cf8\b print}(terms_max(com)[:{\cf4 10}])\par
\par
{\cf8\b def} {\cf19 main2}(fn):\par
    {\cf8\b with} {\cf8 open}(fn {\cf4 +} {\cf2 '}{\cf2 .json}{\cf2 '}, {\cf2 '}{\cf2 r}{\cf2 '}) {\cf8\b as} f:\par
        count_all {\cf4 =} Counter()\par
        com {\cf4 =} defaultdict({\cf8\b lambda} : defaultdict({\cf8 int}))\par
        search_word {\cf4 =} {\cf2 '}{\cf2 '}\par
\par
        {\cf8\b for} line {\cf12\b in} f:\par
            tweet {\cf4 =} json{\cf4 .}loads(line)\par
            tokens {\cf4 =} preprocess(tweet[{\cf2 '}{\cf2 text}{\cf2 '}])\par
            \par
            terms {\cf4 =} terms_filter(tokens)\par
\par
            co_occurrences(terms, com)\par
\par
            terms {\cf4 =} contains_search_word(terms, search_word)\par
            count_all{\cf4 .}update(terms)\par
\par
    word_freq {\cf4 =} count_all{\cf4 .}most_common({\cf4 20})\par
    labels, freq {\cf4 =} {\cf8 zip}({\cf4 *}word_freq)\par
    data {\cf4 =} \{{\cf2 '}{\cf2 data}{\cf2 '}: freq, {\cf2 '}{\cf2 x}{\cf2 '}: labels\}\par
    bar {\cf4 =} vincent{\cf4 .}Bar(data, iter_idx{\cf4 =}{\cf2 '}{\cf2 x}{\cf2 '})\par
    bar{\cf4 .}to_json({\cf2 '}{\cf2 term_freq.json}{\cf2 '})\par
\par
{\cf8\b def} {\cf19 main3}(fn):\par
    {\cf8\b with} {\cf8 open}(fn {\cf4 +} {\cf2 '}{\cf2 .json}{\cf2 '}, {\cf2 '}{\cf2 r}{\cf2 '}) {\cf8\b as} f:\par
        count_all {\cf4 =} Counter()\par
        datess {\cf4 =} []\par
        search_hashes {\cf4 =} [{\cf2 '}{\cf2 #Endomondo}{\cf2 '},{\cf2 '}{\cf2 #MexicoNeedsWWATour}{\cf2 '}]\par
\par
        {\cf8\b for} line {\cf12\b in} f:\par
            tweet {\cf4 =} json{\cf4 .}loads(line)\par
            tokens {\cf4 =} preprocess(tweet[{\cf2 '}{\cf2 text}{\cf2 '}])\par
            \par
            terms {\cf4 =} terms_hash(tokens)\par
            count_all{\cf4 .}update(terms)\par
\par
            {\cf8\b for} i {\cf12\b in} {\cf8 range}({\cf4 0},{\cf8 len}(search_hashes)):\par
                datess{\cf4 .}append([])\par
                {\cf8\b if} search_hashes[i] {\cf12\b in} terms:\par
                    datess[i]{\cf4 .}append(getTime(tweet))\par
\par
    {\cf8\b print}(count_all{\cf4 .}most_common({\cf4 10})) \par
\par
    per_minutes {\cf4 =} []\par
    {\cf8\b for} dates {\cf12\b in} datess:\par
        per_minute {\cf4 =} time_series(dates)\par
        per_minutes{\cf4 .}append(per_minute)\par
\par
    keys {\cf4 =} search_hashes\par
    values {\cf4 =} per_minutes\par
\par
    {\cf5\i # all the data together}\par
    match_data {\cf4 =} {\cf8 dict}({\cf8 zip}(keys,values))\par
    {\cf5\i # we need a DataFrame, to accommodate multiple series}\par
    all_matches {\cf4 =} pandas{\cf4 .}DataFrame(data{\cf4 =}match_data,\par
                                   index{\cf4 =}values[{\cf4 0}]{\cf4 .}index)\par
    {\cf5\i # Resampling as above}\par
    all_matches {\cf4 =} all_matches{\cf4 .}resample({\cf2 '}{\cf2 1Min}{\cf2 '}, how{\cf4 =}{\cf2 '}{\cf2 sum}{\cf2 '}){\cf4 .}fillna({\cf4 0})\par
    {\cf5\i #all_matches = all_matches.resample('1D', how='sum').fillna(0)}\par
\par
    {\cf5\i # and now the plotting}\par
    time_chart {\cf4 =} vincent{\cf4 .}Line(all_matches[keys])\par
    time_chart{\cf4 .}axis_titles(x{\cf4 =}{\cf2 '}{\cf2 Time}{\cf2 '}, y{\cf4 =}{\cf2 '}{\cf2 Freq}{\cf2 '})\par
    time_chart{\cf4 .}legend(title{\cf4 =}{\cf2 '}{\cf2 Matches}{\cf2 '})\par
    time_chart{\cf4 .}to_json(fn {\cf4 +} {\cf2 '}{\cf2 .time_chart.json}{\cf2 '})\par
     \par
    {\cf5\i # time_chart = vincent.Line(per_minute_1)}\par
    {\cf5\i # time_chart.axis_titles(x='Time', y='Freq')}\par
    {\cf5\i # time_chart.to_json('time_chart.json')}\par
\par
{\cf8\b def} {\cf19 main4}(fn):\par
    {\cf8\b with} {\cf8 open}(fn {\cf4 +} {\cf2 '}{\cf2 .json}{\cf2 '}, {\cf2 '}{\cf2 r}{\cf2 '}) {\cf8\b as} f:\par
        count_all {\cf4 =} Counter()\par
        com {\cf4 =} defaultdict({\cf8\b lambda} : defaultdict({\cf8 int}))\par
        n_docs {\cf4 =} {\cf4 0}\par
\par
        {\cf8\b for} line {\cf12\b in} f:\par
            n_docs {\cf4 +}{\cf4 =} {\cf4 1}\par
            tweet {\cf4 =} json{\cf4 .}loads(line)\par
            tokens {\cf4 =} preprocess(tweet[{\cf2 '}{\cf2 text}{\cf2 '}], {\cf8 True})\par
            \par
            terms {\cf4 =} terms_nolink(terms_stop(tokens))\par
\par
            co_occurrences(terms, com)\par
            count_all{\cf4 .}update(terms)\par
\par
    {\cf5\i #print(count_all.most_common(10)) }\par
    semantic_sorted {\cf4 =} sentiment(count_all, com, n_docs, {\cf2 '}{\cf2 positive-words.txt}{\cf2 '}, {\cf2 '}{\cf2 negative-words.txt}{\cf2 '})\par
    top_pos {\cf4 =} semantic_sorted[:{\cf4 10}]\par
    top_neg {\cf4 =} semantic_sorted[{\cf4 -}{\cf4 10}:]\par
\par
    {\cf8\b print}(top_pos)\par
    {\cf8\b print}(top_neg)\par
\par
{\cf8\b def} {\cf19 main5}(fn):\par
    {\cf8\b with} {\cf8 open}(fn {\cf4 +} {\cf2 '}{\cf2 .json}{\cf2 '}, {\cf2 '}{\cf2 r}{\cf2 '}) {\cf8\b as} f:\par
        count_all {\cf4 =} Counter()\par
        dates {\cf4 =} []\par
\par
        {\cf8\b for} line {\cf12\b in} f:\par
            tweet {\cf4 =} json{\cf4 .}loads(line)\par
            tokens {\cf4 =} preprocess(tweet[{\cf2 '}{\cf2 text}{\cf2 '}])\par
            \par
            terms {\cf4 =} terms_hash(tokens)\par
            count_all{\cf4 .}update(terms)\par
\par
            time {\cf4 =} getTime(tweet)\par
            {\cf8\b if} time {\cf12\b is} {\cf12\b not} {\cf8 None}: dates{\cf4 .}append(time)\par
\par
    {\cf8\b print}(count_all{\cf4 .}most_common({\cf4 10})) \par
\par
    per_minute {\cf4 =} time_series(dates, {\cf2 '}{\cf2 1H}{\cf2 '})\par
    per_minute{\cf4 .}to_csv(fn {\cf4 +} {\cf2 '}{\cf2 .series.csv}{\cf2 '}, sep{\cf4 =}{\cf2 '}{\cf10\b \\t}{\cf2 '}, encoding{\cf4 =}{\cf2 '}{\cf2 utf-8}{\cf2 '})\par
\par
    {\cf5\i # and now the plotting}\par
    time_chart {\cf4 =} vincent{\cf4 .}Line(per_minute)\par
    time_chart{\cf4 .}axis_titles(x{\cf4 =}{\cf2 '}{\cf2 Time}{\cf2 '}, y{\cf4 =}{\cf2 '}{\cf2 Freq}{\cf2 '})\par
    time_chart{\cf4 .}to_json(fn {\cf4 +} {\cf2 '}{\cf2 .count.time_chart.json}{\cf2 '})\par
\par
{\cf8\b if} __name__ {\cf4 ==} {\cf2 '}{\cf2 __main__}{\cf2 '}:\par
    {\cf5\i #fn = 'test.sample.en'}\par
    fn {\cf4 =} {\cf2 '}{\cf2 test.en}{\cf2 '}\par
    {\cf5\i #main(fn)}\par
    {\cf5\i #main2(fn)}\par
    {\cf5\i #main3(fn)}\par
    {\cf5\i #main4(fn)}\par
    main5(fn)\par
}